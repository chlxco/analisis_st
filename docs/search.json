[{"path":"index.html","id":"propuesta","chapter":"Capitulo 1 Propuesta","heading":"Capitulo 1 Propuesta","text":"Análisis de riesgo crediticio y su importancia La evaluación del riesgo crediticio es una tarea crítica para cualquier empresa que ofrezca préstamos o créditos.\nEl incumplimiento de los términos del préstamo o la falta de pago pueden generar grandes pérdidas financieras y afectar la estabilidad de la empresa.\nEs por ello que es importante contar con herramientas y técnicas que permitan evaluar el riesgo crediticio de manera eficiente.El análisis de riesgo crediticio utiliza históricos para evaluar el comportamiento de los clientes lo largo de varios periodos.\nDe esta forma, se pueden obtener patrones y características que permiten segregar grupos de clientes y determinar un posible perfil de riesgo.\nLos resultados obtenidos partir de este análisis pueden ayudar la empresa tomar decisiones más informadas y reducir el riesgo crediticio.Además, el análisis de riesgo crediticio permite identificar posibles oportunidades para la empresa.\nPor ejemplo, puede ayudar la empresa identificar clientes que presenten un bajo riesgo crediticio y, por lo tanto, puedan recibir préstamos con tasas de interés más bajas.\nDe esta manera, la empresa puede aumentar su base de clientes y mejorar su rentabilidad.En cuanto las fuentes de información, se pueden utilizar diversas fuentes para recopilar los datos necesarios para el análisis de riesgo crediticio.\nEntre ellas se encuentran bases de datos públicas y privadas, encuestas, registros gubernamentales, entre otras.\nEs importante verificar la calidad de la información obtenida para asegurar la precisión y confiabilidad de los resultados.En el caso específico de la empresa ABC, se cuenta con los permisos necesarios por parte del área de gestión de cobranzas y se eliminaron los datos personales de los clientes con los cuales se extrajeron las características.En resumen, el análisis de riesgo crediticio es una técnica muy útil para evaluar el riesgo crediticio de los clientes y para identificar posibles oportunidades para la empresa.\nSe pueden utilizar diversas fuentes de información para recopilar los datos necesarios para este análisis, pero es importante asegurarse de contar con los permisos necesarios y verificar la calidad de la información obtenida.","code":""},{"path":"Analisis.html","id":"Analisis","chapter":"Capitulo 2 Analisis Exploratorio","heading":"Capitulo 2 Analisis Exploratorio","text":"","code":""},{"path":"Analisis.html","id":"introducción","chapter":"Capitulo 2 Analisis Exploratorio","heading":"2.1 Introducción","text":"En este análisis, se explorará el comportamiento de la utilización de los productos de un banco lo largo de los años. Se examinará si ha habido un incremento en la utilización de los productos año año, y en caso afirmativo, se intentará identificar cuáles son las razones que han llevado este aumento.En particular, se examinará la utilización de los productos de un banco desde el año 2018 hasta el año en curso. Se considerarán productos tales como tarjetas de crédito y crédito de consumo. Se investigará si existe un patrón estacional en la utilización de estos productos, es decir, si hay un comportamiento cíclico que se repite en determinados meses del año. Además, se estudiará la posible existencia de tendencias largo plazo que puedan indicar un cambio en el comportamiento de los clientes del banco y que posibles factores indicen en estos cambios.La metodología utilizada en este análisis incluye la descomposición de series de tiempo, la identificación de patrones estacionales y la realización de pruebas estadísticas para verificar la presencia de tendencias y cambios en el comportamiento de los clientes.","code":""},{"path":"Analisis.html","id":"explicación-de-la-base","chapter":"Capitulo 2 Analisis Exploratorio","heading":"2.2 Explicación de la base","text":"Partimos de una base agrupada con las siguientes variables:Periodo - > año y mes\nSub_Tipo -> tipo de producto\nN_Clientes -> cantidad de cleintes\nDIAS_DE_MORA -> días de mora de los clientes cierre de mes\nSaldo -> saldo utilizado por los clientes cierre de mes\nGenero -> genero del cliente\ngrupo_actividad_eco -> que actividad económica tiene el grupo de clienes\nCuidad_res -> ciudad de residencia de los clientes","code":""},{"path":"Analisis.html","id":"creacion-del-objeto-de-analisis-temporal-indice.ts","chapter":"Capitulo 2 Analisis Exploratorio","heading":"2.3 Creacion del objeto de analisis temporal indice.ts","text":"","code":""},{"path":"Analisis.html","id":"carga-de-librerias-y-datasource","chapter":"Capitulo 2 Analisis Exploratorio","heading":"2.3.1 Carga de librerias y datasource","text":"","code":"## Registered S3 method overwritten by 'quantmod':\n##   method            from\n##   as.zoo.data.frame zoo## Loading required package: zoo## \n## Attaching package: 'zoo'## The following objects are masked from 'package:base':\n## \n##     as.Date, as.Date.numeric## Successfully loaded changepoint package version 2.2.4\n##  See NEWS for details of changes.## # A tibble: 643,570 × 8\n##    Periodo Sub_Tipo N_Clientes DIAS_DE_MORA     Saldo Genero grupo_actividad_eco\n##    <chr>   <chr>         <dbl>        <dbl>     <dbl> <chr>  <chr>              \n##  1 2018-01 CDC               4            0 15824105. Femen… Dependiente privado\n##  2 2018-01 CDC               1            0  6810373. Femen… Dependiente privado\n##  3 2018-01 CDC               6            6 28819502. Femen… Dependiente privado\n##  4 2018-01 CDC              12           63 81343674. Femen… Dependiente privado\n##  5 2018-01 CDC               1           21  7524344. Femen… Dependiente privado\n##  6 2018-01 CDC               4            0 12974213. Femen… Dependiente privado\n##  7 2018-01 CDC               3            1 21348609. Femen… Dependiente privado\n##  8 2018-01 CDC               2            0 11475858. Femen… Dependiente privado\n##  9 2018-01 CDC              10           38 60012355. Femen… Dependiente privado\n## 10 2018-01 CDC               1            0  9034715. Femen… Dependiente privado\n## # ℹ 643,560 more rows\n## # ℹ 1 more variable: Cuidad_res <chr>"},{"path":"Analisis.html","id":"modificamos-el-df-para-que-tenga-el-formato-adecuado-y-lo-mostramos","chapter":"Capitulo 2 Analisis Exploratorio","heading":"2.3.2 Modificamos el df para que tenga el formato adecuado y lo mostramos","text":"Deacuerdo al analisis que deseamos hacer consolidamos nuestra variable de interes saldo agrupandola por periodo de tiempo y procedimos graficar","code":"##               Jan          Feb          Mar          Apr          May\n## 2018 2.562223e+12 2.601532e+12 2.653315e+12 2.717915e+12 2.852608e+12\n## 2019 3.732137e+12 3.810740e+12 3.923380e+12 3.995810e+12 4.092819e+12\n## 2020 2.022405e+12 4.295886e+12 4.304185e+12 4.195404e+12 4.177219e+12\n## 2021 4.369300e+12 4.478002e+12 4.595047e+12 4.627874e+12 4.705783e+12\n## 2022 6.351501e+12 6.555819e+12 6.739823e+12 6.925001e+12 7.132542e+12\n## 2023 8.083445e+12 7.951094e+12 7.812947e+12 7.765132e+12             \n##               Jun          Jul          Aug          Sep          Oct\n## 2018 2.986446e+12 3.102493e+12 3.196138e+12 3.272388e+12 3.394038e+12\n## 2019 4.164386e+12 4.198177e+12 4.267921e+12 4.307340e+12 4.113506e+12\n## 2020 4.164434e+12 4.082507e+12 4.067948e+12 4.120926e+12 4.182877e+12\n## 2021 4.846473e+12 4.983882e+12 5.091594e+12 5.385784e+12 5.730839e+12\n## 2022 7.435792e+12 7.529328e+12 7.670212e+12 7.789046e+12 7.903472e+12\n## 2023                                                                 \n##               Nov          Dec\n## 2018 3.617129e+12 3.690229e+12\n## 2019 4.314034e+12 4.455499e+12\n## 2020 4.344709e+12 4.384188e+12\n## 2021 5.955740e+12 6.164705e+12\n## 2022 8.115444e+12 8.154174e+12\n## 2023"},{"path":"Analisis.html","id":"analisis-grafica-de-serie","chapter":"Capitulo 2 Analisis Exploratorio","heading":"2.3.3 Analisis Grafica de serie","text":"Después de graficar la serie de tiempo, podemos observar ciertas características que nos brindan información valiosa. Por ejemplo, en el primer periodo del 2020, podemos notar un pico descendente en el saldo, lo que podría sugerir un posible error en el registro de los datos históricos.Por otro lado, al examinar el comportamiento general de la serie de tiempo, se observa un aumento en la utilización de los productos lo largo de los años, especialmente marcado partir de la mitad del 2020. Este aumento podría deberse factores como la pandemia y el desempleo, que podrían haber influido en la demanda de estos productos.","code":""},{"path":"Analisis.html","id":"chequeos-basicos-para-confirmar-la-estructura-del-contenedor-ts","chapter":"Capitulo 2 Analisis Exploratorio","heading":"2.3.4 Chequeos basicos para confirmar la estructura del contenedor ts","text":"","code":"## [1] \"El tipo de datos del df indice.ts es: \"## [1] \"ts\"## [1] \"La serie de tiempo indice.ts empieza en: \"## [1] 2018    1## [1] \"La serie de tiempo indice.ts termina en: \"## [1] 2023    4"},{"path":"Analisis.html","id":"analisis-descriptivo","chapter":"Capitulo 2 Analisis Exploratorio","heading":"2.4 Analisis Descriptivo","text":"","code":""},{"path":"Analisis.html","id":"grafica-de-rezagos","chapter":"Capitulo 2 Analisis Exploratorio","heading":"2.4.1 Grafica de Rezagos","text":"","code":""},{"path":"Analisis.html","id":"media-movil","chapter":"Capitulo 2 Analisis Exploratorio","heading":"2.4.2 Media Movil","text":"Crearemos continuacion 3 medias moviles para el objeto ts. Estas tendran 3, 5 y 7 periodos para su calculo.Veamos como es el comportamiento de las mismas en comparacion con los datos originales de la serie de tiempo.","code":"## Media Movil con 3 meses:  2.60569e+12 2.657587e+12 2.741279e+12 2.852323e+12 2.980516e+12 3.095026e+12 3.19034e+12 3.287521e+12 3.427851e+12 3.567132e+12 3.679831e+12 3.744369e+12 3.822086e+12 3.909977e+12 4.004003e+12 4.084338e+12 4.151794e+12 4.210161e+12 4.257812e+12 4.229589e+12 4.24496e+12 4.294346e+12 3.597313e+12 3.591264e+12 3.540826e+12 4.265159e+12 4.225603e+12 4.179019e+12 4.141387e+12 4.104963e+12 4.090461e+12 4.123917e+12 4.216171e+12 4.303925e+12 4.366066e+12 4.410497e+12 4.480783e+12 4.566974e+12 4.642901e+12 4.72671e+12 4.845379e+12 4.973983e+12 5.153753e+12 5.402739e+12 5.690788e+12 5.950428e+12 6.157316e+12 6.357342e+12 6.549048e+12 6.740214e+12 6.932456e+12 7.164445e+12 7.365887e+12 7.545111e+12 7.662862e+12 7.787577e+12 7.935987e+12 8.057696e+12 8.117688e+12 8.062904e+12 7.949162e+12 7.843058e+12## Media Movil con 5 meses:  2.677519e+12 2.762363e+12 2.862556e+12 2.97112e+12 3.082015e+12 3.190301e+12 3.316437e+12 3.433984e+12 3.541184e+12 3.648854e+12 3.754723e+12 3.830459e+12 3.910977e+12 3.997427e+12 4.074914e+12 4.143823e+12 4.206128e+12 4.210266e+12 4.240195e+12 4.29166e+12 3.842557e+12 3.840266e+12 3.878402e+12 3.854676e+12 3.79902e+12 4.227426e+12 4.18475e+12 4.137503e+12 4.122607e+12 4.123739e+12 4.159794e+12 4.22013e+12 4.2804e+12 4.351815e+12 4.434249e+12 4.490882e+12 4.555201e+12 4.650636e+12 4.751812e+12 4.851121e+12 5.002703e+12 5.207714e+12 5.429568e+12 5.665732e+12 5.917714e+12 6.151721e+12 6.353518e+12 6.54737e+12 6.740937e+12 6.957795e+12 7.152497e+12 7.338575e+12 7.511384e+12 7.66557e+12 7.8015e+12 7.92647e+12 8.009116e+12 8.041526e+12 8.023421e+12 7.953359e+12## Media Movil con 7 meses:  2.782362e+12 2.872921e+12 2.968758e+12 3.074575e+12 3.203034e+12 3.322694e+12 3.429222e+12 3.5304e+12 3.634291e+12 3.737637e+12 3.837463e+12 3.915643e+12 3.988207e+12 4.064748e+12 4.13569e+12 4.162851e+12 4.208312e+12 4.260123e+12 3.954126e+12 3.968084e+12 3.973265e+12 3.957274e+12 3.966376e+12 3.945005e+12 3.89172e+12 4.183941e+12 4.158946e+12 4.141617e+12 4.162946e+12 4.192513e+12 4.221779e+12 4.278279e+12 4.353579e+12 4.426e+12 4.5007e+12 4.572381e+12 4.658052e+12 4.761236e+12 4.89092e+12 5.053176e+12 5.242871e+12 5.451288e+12 5.666292e+12 5.890855e+12 6.126316e+12 6.346204e+12 6.546447e+12 6.757883e+12 6.952829e+12 7.141217e+12 7.317392e+12 7.483628e+12 7.653691e+12 7.799638e+12 7.89216e+12 7.952412e+12 7.972803e+12 7.969387e+12"},{"path":"estacionalidad-y-descomposicion.html","id":"estacionalidad-y-descomposicion","chapter":"Capitulo 3 Estacionalidad y Descomposicion","heading":"Capitulo 3 Estacionalidad y Descomposicion","text":"","code":""},{"path":"estacionalidad-y-descomposicion.html","id":"Estacion","chapter":"Capitulo 3 Estacionalidad y Descomposicion","heading":"3.1 Estacionalidad","text":"","code":""},{"path":"estacionalidad-y-descomposicion.html","id":"analisis-inicial","chapter":"Capitulo 3 Estacionalidad y Descomposicion","heading":"3.1.1 Analisis Inicial","text":"Según el análisis de estacionalidad anual, se observa que la utilización de los productos del banco fue moderada en los años 2018 y 2019, tal como se evidencia en las líneas trazadas para esos años. En el 2020, se observa un pico en la utilización que podría indicar un posible error en la recolección de datos. Posteriormente, se observa una pequeña disminución que posiblemente se debió al inicio de la pandemia y la incertidumbre mundial. partir de septiembre de 2020, se observa un incremento continuo en la utilización para los años 2021 y 2022, posiblemente como resultado de la duración de la pandemia y la crisis económica global.En el año 2023, se comienza evidenciar una disminución en la utilización de los productos del banco, lo que podría deberse al alza de las tasas de interés o un cambio en el comportamiento de los clientes. Es importante mencionar que se requiere un análisis más detallado para determinar las causas precisas de esta disminución.","code":""},{"path":"estacionalidad-y-descomposicion.html","id":"descomposicion-del-objeto-y-analisis","chapter":"Capitulo 3 Estacionalidad y Descomposicion","heading":"3.2 Descomposicion del objeto y analisis","text":"pesar de presentar un patron recurrente en el componente de estacionalidad, se puede observar un trend en la serie de datos. De igual manera el error se ve aleatorio sino que por el contrario, presenta un patron constante. Dicho esto, vamos comprobar por medio del Augmented Dickery-Fuller test (adf) la estacionalidad del conjunto de datos.","code":""},{"path":"estacionalidad-y-descomposicion.html","id":"prueba-de-estacionalidad","chapter":"Capitulo 3 Estacionalidad y Descomposicion","heading":"3.2.1 Prueba de Estacionalidad","text":"De acuerdo al resultado (p-value >0.05), debemos aceptar la H0 la cual nos confirma la -estacionalidad del conjunto. Esto quiere decir que el objeto indice.ts requerira una transformacion para su posterior procesamiento en el modelo.","code":"## \n##  Augmented Dickey-Fuller Test\n## \n## data:  indice.ts\n## Dickey-Fuller = -1.2017, Lag order = 3, p-value = 0.8984\n## alternative hypothesis: stationary"},{"path":"estacionalidad-y-descomposicion.html","id":"autocorrelacion","chapter":"Capitulo 3 Estacionalidad y Descomposicion","heading":"3.2.2 Autocorrelacion","text":"Ahora veamos si existe autocorrelacion total o parcial.(acf y pacf tests)Como se puede observar, existe autocorrelacion entre la variable observada lo cual confirma la tendencia en la serie temporal. Por otro lado evidenciamos autocorrelacion parcial ya que encontramos picos por fuera del umbral (0.95)","code":""},{"path":"estacionalidad-y-descomposicion.html","id":"transformacion","chapter":"Capitulo 3 Estacionalidad y Descomposicion","heading":"3.3 Transformacion","text":"","code":"## [1] \"Ajustamos la estacionalidad de la serie de tiempo por medio del comando seasadj\"## [1] \"Luego removemos la tendencia (trend) con el comando diff\"## [1] \"Grafiquemos la nueva serie de tiempo\""},{"path":"estacionalidad-y-descomposicion.html","id":"validacion-de-nuestra-nueva-serie-de-tiempo-transformada","chapter":"Capitulo 3 Estacionalidad y Descomposicion","heading":"3.3.1 Validacion de nuestra nueva serie de tiempo transformada","text":"se observa que la serie es de tipo estacionaria (p-value=0.0316 < 0.05), la varianza y la media son de tipo constante, los datos se mueven alrededor de cero (0).","code":"## \n##  Augmented Dickey-Fuller Test\n## \n## data:  modelo_ts\n## Dickey-Fuller = -3.7052, Lag order = 3, p-value = 0.0316\n## alternative hypothesis: stationary"},{"path":"estacionalidad-y-descomposicion.html","id":"conclusiones","chapter":"Capitulo 3 Estacionalidad y Descomposicion","heading":"3.4 Conclusiones","text":"En conclusión, el análisis de serie de tiempo de la utilización de los productos del banco nos ha permitido identificar patrones y tendencias en su utilización.partir del análisis de estacionalidad, se observó que la utilización de los productos del banco ha sido moderada en los años 2018 y 2019, y ha aumentado significativamente en el 2020 y 2021. Además, se evidenció una disminución en la utilización para el año 2023.La descomposición de la serie de tiempo nos permitió identificar las componentes de tendencia, estacionalidad y error, y su análisis nos ha brindado una mejor comprensión de los patrones y comportamientos observados en la utilización de los productos del banco.Asimismo, se realizó la diferenciación de la serie de tiempo para eliminar la tendencia y hacer la serie estacionaria. Esto nos permitió obtener una serie de tiempo más homogénea, y por lo tanto, una mejor visualización de las fluctuaciones en la utilización de los productos del banco.Como resultado, la nueva serie de tiempo sera nuestra base para la implementacion de modelos de pronostico de los productos observados.","code":""},{"path":"holt-wintershw-y-suavizamiento-exp..html","id":"holt-wintershw-y-suavizamiento-exp.","chapter":"Capitulo 4 Holt-Winters(HW) y Suavizamiento Exp.","heading":"Capitulo 4 Holt-Winters(HW) y Suavizamiento Exp.","text":"","code":""},{"path":"holt-wintershw-y-suavizamiento-exp..html","id":"hw","chapter":"Capitulo 4 Holt-Winters(HW) y Suavizamiento Exp.","heading":"4.1 HW","text":"Para aplicar la metodología de Holt-Winters y suavizamiento exponencial la serie de tiempo indice.ts, podemos utilizar las funciones correspondientes de R. La metodología de Holt-Winters es adecuada para modelar series de tiempo con componentes de tendencia y estacionalidad.\nEl resultado del modelo Holt-Winters proporciona las componentes de tendencia (trend), estacionalidad (seasonal) y residuos (residuals). Estas componentes ayudan comprender los patrones y la estructura de la serie de tiempo.continuación, se muestra cómo aplicar ambas técnicas la serie de tiempo indice.ts:Suavizamiento exponencial:\nEl suavizamiento exponencial,es útil para suavizar las fluctuaciones en la serie de tiempo.La función HoltWinters con los argumentos beta = FALSE y gamma = FALSE aplica el suavizamiento exponencial simple sin considerar la componente de estacionalidad en el modelo.Al aplicar esta metodología y el suavizamiento exponencial la serie de tiempo indice.ts, se obtendrá la tendencia, la estacionalidad, los residuos y la serie de tiempo suavizada. Estos resultados ayudarán comprender la estructura y los patrones de la serie de tiempo.El método Holt Winters nos permite realizar predicciones utilizando la serie de tiempo. continuación, se muestra el proceso de generación de predicciones.Se generan predicciones para los próximos 12 meses (May 2023 - Apr 2024) y se procede graficar la predicción.Tambien podemos utilizar la funcion hwse generan predicciones para los próximos 12 periodos.Esto generará un gráfico que muestra la serie de tiempo original y las predicciones del modelo de Holt-Winters.","code":"\n# Aplicar la metodología de Holt-Winters\nhw_model <- HoltWinters(indice.ts)\n\n# Obtener las componentes del modelo (tendencia, estacionalidad y residuos)\ntrend <- hw_model$components$trend\nseasonal <- hw_model$components$seasonal\nresiduals <- hw_model$components$random\n\n# Imprimir las componentes\nprint(\"Tendencia:\")## [1] \"Tendencia:\"\nprint(trend)## NULL\nprint(\"Estacionalidad:\")## [1] \"Estacionalidad:\"\nprint(seasonal)## NULL\nprint(\"Residuos:\")## [1] \"Residuos:\"\nprint(residuals)## NULL\n# Aplicar suavizamiento exponencial\nsmoothed <- HoltWinters(indice.ts, beta = FALSE, gamma = FALSE)$fitted\n\n# Imprimir la serie de tiempo suavizada\nprint(\"Serie de tiempo suavizada:\")## [1] \"Serie de tiempo suavizada:\"\nprint(smoothed)##                  xhat        level\n## Feb 2018 2.562223e+12 2.562223e+12\n## Mar 2018 2.586847e+12 2.586847e+12\n## Apr 2018 2.628483e+12 2.628483e+12\n## May 2018 2.684505e+12 2.684505e+12\n## Jun 2018 2.789807e+12 2.789807e+12\n## Jul 2018 2.912985e+12 2.912985e+12\n## Aug 2018 3.031696e+12 3.031696e+12\n## Sep 2018 3.134705e+12 3.134705e+12\n## Oct 2018 3.220952e+12 3.220952e+12\n## Nov 2018 3.329376e+12 3.329376e+12\n## Dec 2018 3.509629e+12 3.509629e+12\n## Jan 2019 3.622760e+12 3.622760e+12\n## Feb 2019 3.691275e+12 3.691275e+12\n## Mar 2019 3.766110e+12 3.766110e+12\n## Apr 2019 3.864626e+12 3.864626e+12\n## May 2019 3.946802e+12 3.946802e+12\n## Jun 2019 4.038270e+12 4.038270e+12\n## Jul 2019 4.117271e+12 4.117271e+12\n## Aug 2019 4.167952e+12 4.167952e+12\n## Sep 2019 4.230574e+12 4.230574e+12\n## Oct 2019 4.278661e+12 4.278661e+12\n## Nov 2019 4.175205e+12 4.175205e+12\n## Dec 2019 4.262169e+12 4.262169e+12\n## Jan 2020 4.383274e+12 4.383274e+12\n## Feb 2020 2.904388e+12 2.904388e+12\n## Mar 2020 3.776045e+12 3.776045e+12\n## Apr 2020 4.106880e+12 4.106880e+12\n## May 2020 4.162333e+12 4.162333e+12\n## Jun 2020 4.171658e+12 4.171658e+12\n## Jul 2020 4.167133e+12 4.167133e+12\n## Aug 2020 4.114122e+12 4.114122e+12\n## Sep 2020 4.085198e+12 4.085198e+12\n## Oct 2020 4.107579e+12 4.107579e+12\n## Nov 2020 4.154747e+12 4.154747e+12\n## Dec 2020 4.273742e+12 4.273742e+12\n## Jan 2021 4.342927e+12 4.342927e+12\n## Feb 2021 4.359448e+12 4.359448e+12\n## Mar 2021 4.433712e+12 4.433712e+12\n## Apr 2021 4.534775e+12 4.534775e+12\n## May 2021 4.593094e+12 4.593094e+12\n## Jun 2021 4.663684e+12 4.663684e+12\n## Jul 2021 4.778186e+12 4.778186e+12\n## Aug 2021 4.907038e+12 4.907038e+12\n## Sep 2021 5.022646e+12 5.022646e+12\n## Oct 2021 5.250122e+12 5.250122e+12\n## Nov 2021 5.551251e+12 5.551251e+12\n## Dec 2021 5.804630e+12 5.804630e+12\n## Jan 2022 6.030187e+12 6.030187e+12\n## Feb 2022 6.231463e+12 6.231463e+12\n## Mar 2022 6.434645e+12 6.434645e+12\n## Apr 2022 6.625813e+12 6.625813e+12\n## May 2022 6.813229e+12 6.813229e+12\n## Jun 2022 7.013252e+12 7.013252e+12\n## Jul 2022 7.277938e+12 7.277938e+12\n## Aug 2022 7.435413e+12 7.435413e+12\n## Sep 2022 7.582495e+12 7.582495e+12\n## Oct 2022 7.711882e+12 7.711882e+12\n## Nov 2022 7.831897e+12 7.831897e+12\n## Dec 2022 8.009515e+12 8.009515e+12\n## Jan 2023 8.100132e+12 8.100132e+12\n## Feb 2023 8.089679e+12 8.089679e+12\n## Mar 2023 8.002867e+12 8.002867e+12\n## Apr 2023 7.883898e+12 7.883898e+12\nhw_model=HoltWinters(indice.ts, seasonal = \"additive\")\nplot(hw_model)\nplot(fitted(hw_model))\npred=predict(hw_model, 12, prediction.interval = TRUE)\npred##                   fit          upr          lwr\n## May 2023 7.958686e+12 8.802116e+12 7.115255e+12\n## Jun 2023 7.938280e+12 8.890571e+12 6.985989e+12\n## Jul 2023 7.825482e+12 8.920472e+12 6.730492e+12\n## Aug 2023 7.788949e+12 9.056689e+12 6.521209e+12\n## Sep 2023 7.730847e+12 9.197373e+12 6.264322e+12\n## Oct 2023 7.715284e+12 9.403168e+12 6.027400e+12\n## Nov 2023 7.802406e+12 9.731469e+12 5.873343e+12\n## Dec 2023 7.743702e+12 9.931634e+12 5.555770e+12\n## Jan 2024 7.659827e+12 1.012267e+13 5.196985e+12\n## Feb 2024 7.617072e+12 1.036958e+13 4.864568e+12\n## Mar 2024 7.610883e+12 1.066677e+13 4.554994e+12\n## Apr 2024 7.579164e+12 1.095133e+13 4.207002e+12\nplot(hw_model, pred)\n# Calcular el modelo de Holt-Winters utilizando la función hw():\n  \nhw_model1 <- HoltWinters(indice.ts)\nhw_model1## Holt-Winters exponential smoothing with trend and additive seasonal component.\n## \n## Call:\n## HoltWinters(x = indice.ts)\n## \n## Smoothing parameters:\n##  alpha: 0.407578\n##  beta : 0.2861578\n##  gamma: 0\n## \n## Coefficients:\n##              [,1]\n## a   7949360142242\n## b    -31047949847\n## s1    40373454460\n## s2    51015741884\n## s3   -30734639955\n## s4   -36219767637\n## s5   -63272955789\n## s6   -47788675079\n## s7    70381305327\n## s8    42725269292\n## s9   -10101155804\n## s10  -21808809060\n## s11    3050492943\n## s12    2379739417\n# Obtener las predicciones del modelo para un horizonte de tiempo determinado:\nlibrary(forecast) \npredictions <- forecast(hw_model1, h = 12)  # Predicciones para los próximos 12 periodos\nplot(hw_model1, main = \"Modelo de Holt-Winters: Serie de tiempo y predicciones\")\nlines(predictions$mean, col = \"blue\")\nlegend(\"bottomright\", legend = c(\"Serie de tiempo\", \"Predicciones\"), col = c(\"black\", \"blue\"), lty = 1)"},{"path":"modelo-arima.html","id":"modelo-arima","chapter":"Capitulo 5 Modelo ARIMA","heading":"Capitulo 5 Modelo ARIMA","text":"##Metodología Box-Jenkins para identificar modelos autoregresivos integrados de media móvil (ARIMA) para analizar y predecir valores futuros de serie de tiempo.En esta seccion, intentaremos abordar el algoritmo ARIMA dentro de la serie de tiempo. Primero consrtruiremos el modelo optimo AR, luego MA y posteriormente utilizaremos la funcion Autoarima para encontrar los parametros optimos.","code":""},{"path":"modelo-arima.html","id":"modelos","chapter":"Capitulo 5 Modelo ARIMA","heading":"5.1 Modelos","text":"Como se puede observar, se rechaza la hipotesis ho y aceptamos la alterna. xxxxxxx , dentro de nuestros modelos ARIMA podemos asegurar que el parametro d = 0.","code":"## ── Attaching packages ────────────────────────────────────────────── fpp2 2.5 ──## ✔ fma       2.5     ✔ expsmooth 2.3## ## [1] \"Verificamos la estacionalidad del modelo (p<0.05)\"## \n##  Augmented Dickey-Fuller Test\n## \n## data:  modelo_ts\n## Dickey-Fuller = -3.7052, Lag order = 3, p-value = 0.0316\n## alternative hypothesis: stationary"},{"path":"modelo-arima.html","id":"modelo-basado-solamente-en-auro-regresion-ar.-debemos-ubicar-los-parametros-d-y-q-en-0.","chapter":"Capitulo 5 Modelo ARIMA","heading":"5.1.1 Modelo basado solamente en Auro Regresion (AR). Debemos ubicar los parametros d y q en 0.","text":"Por medio del analisis ACF y PACF verificamos los lagsEn el grafico PAFC podemos ver los lag en 1 como punto significativo de cambio.Construyamos entonces nuestro modelo con valor AR (p) = 1. tener en cuenta: Al tener un modelo diferenciado, debemos especificar que se incluya la media en los calculos ya que la misma es 0.","code":"## [1] 0## \n## Call:\n## arima(x = estacion.ts, order = c(1, 0, 0), include.mean = F)\n## \n## Coefficients:\n##           ar1\n##       -0.4010\n## s.e.   0.1139\n## \n## sigma^2 estimated as 1.621e+23:  log likelihood = -1772.91,  aic = 3549.83"},{"path":"modelo-arima.html","id":"modelo-basado-solamente-en-el-moving-average-ma.-parametros-p-y-d-en-0","chapter":"Capitulo 5 Modelo ARIMA","heading":"5.1.2 Modelo basado solamente en el Moving Average (MA). Parametros p y d en 0","text":"Utilizando el grafico ACF revisamos cual seria el punto de inflexion y luego procedemos crear nuestro modelo","code":"## [1] \"Observamos que el 2do lag contiene el ultimo cambio significativo. Ademas el 1ero es descartable siempre ya que es comparable solo consigo mismo.\"## \n## Call:\n## arima(x = estacion.ts, order = c(0, 0, 2), include.mean = F)\n## \n## Coefficients:\n##           ma1     ma2\n##       -0.5263  0.2126\n## s.e.   0.1412  0.1260\n## \n## sigma^2 estimated as 1.556e+23:  log likelihood = -1771.7,  aic = 3549.39"},{"path":"modelo-arima.html","id":"modelo-arima.-validacion-por-medio-de-la-funcion-auto.arima","chapter":"Capitulo 5 Modelo ARIMA","heading":"5.2 Modelo ARIMA. Validacion por medio de la funcion auto.arima","text":"Como pudimos comprobar en los puntos anteriores, la seleccion de parametros del modelo clasico Arima, depende de las caracteristicas de la serie de tiempo evaluar. Es por ello que decidimos comparar el calculo manual de nuestras variables con respecto al modelo automatico que viene incluido en la libreria de forecast auto.arima.Para mostrar los resultados, debemos habilitar la opcion trace la cual permite evaluar todos los modelos que pudiesen resultar de la serie de tiempo. Asi mismo, decidimos utilizar 2 parametros mas y configurarlos en Falso - Stepwise y Approximation - los cuales maximizan la busqueda del mejor modelo, al tiempo que sacrifica tanto numero de modelos evaluar asi como velocidad de respuesta.Como resultado, podemos concluir que el Modelo ARIMA mas optimo para nuestea serie de datos es el que utiliza un AR = 1, MA = 2 y 0 en su atributo diferenciador.","code":"## \n##  ARIMA(0,0,0)            with zero mean     : 3559.137\n##  ARIMA(0,0,0)(0,0,1)[12] with zero mean     : 3561.092\n##  ARIMA(0,0,0)(1,0,0)[12] with zero mean     : 3561.089\n##  ARIMA(0,0,0)(1,0,1)[12] with zero mean     : 3563.321\n##  ARIMA(0,0,1)            with zero mean     : 3550.238\n##  ARIMA(0,0,1)(0,0,1)[12] with zero mean     : 3551.857\n##  ARIMA(0,0,1)(1,0,0)[12] with zero mean     : 3551.834\n##  ARIMA(0,0,1)(1,0,1)[12] with zero mean     : Inf\n##  ARIMA(0,0,2)            with zero mean     : 3549.801\n##  ARIMA(0,0,2)(0,0,1)[12] with zero mean     : 3551.52\n##  ARIMA(0,0,2)(1,0,0)[12] with zero mean     : 3551.505\n##  ARIMA(0,0,2)(1,0,1)[12] with zero mean     : Inf\n##  ARIMA(0,0,3)            with zero mean     : 3549.69\n##  ARIMA(0,0,3)(0,0,1)[12] with zero mean     : 3551.74\n##  ARIMA(0,0,3)(1,0,0)[12] with zero mean     : 3551.735\n##  ARIMA(0,0,3)(1,0,1)[12] with zero mean     : 3554.18\n##  ARIMA(0,0,4)            with zero mean     : 3551.745\n##  ARIMA(0,0,4)(0,0,1)[12] with zero mean     : 3553.955\n##  ARIMA(0,0,4)(1,0,0)[12] with zero mean     : 3553.952\n##  ARIMA(0,0,5)            with zero mean     : 3553.08\n##  ARIMA(1,0,0)            with zero mean     : 3550.027\n##  ARIMA(1,0,0)(0,0,1)[12] with zero mean     : 3551.701\n##  ARIMA(1,0,0)(1,0,0)[12] with zero mean     : 3551.685\n##  ARIMA(1,0,0)(1,0,1)[12] with zero mean     : Inf\n##  ARIMA(1,0,1)            with zero mean     : 3551.278\n##  ARIMA(1,0,1)(0,0,1)[12] with zero mean     : 3552.931\n##  ARIMA(1,0,1)(1,0,0)[12] with zero mean     : 3552.909\n##  ARIMA(1,0,1)(1,0,1)[12] with zero mean     : Inf\n##  ARIMA(1,0,2)            with zero mean     : 3544.017\n##  ARIMA(1,0,2)(0,0,1)[12] with zero mean     : 3546.351\n##  ARIMA(1,0,2)(1,0,0)[12] with zero mean     : 3546.354\n##  ARIMA(1,0,2)(1,0,1)[12] with zero mean     : 3548.724\n##  ARIMA(1,0,3)            with zero mean     : 3546.198\n##  ARIMA(1,0,3)(0,0,1)[12] with zero mean     : 3548.603\n##  ARIMA(1,0,3)(1,0,0)[12] with zero mean     : 3548.607\n##  ARIMA(1,0,4)            with zero mean     : 3548.636\n##  ARIMA(2,0,0)            with zero mean     : 3550.466\n##  ARIMA(2,0,0)(0,0,1)[12] with zero mean     : 3552.099\n##  ARIMA(2,0,0)(1,0,0)[12] with zero mean     : 3552.075\n##  ARIMA(2,0,0)(1,0,1)[12] with zero mean     : Inf\n##  ARIMA(2,0,1)            with zero mean     : 3552.408\n##  ARIMA(2,0,1)(0,0,1)[12] with zero mean     : 3554.165\n##  ARIMA(2,0,1)(1,0,0)[12] with zero mean     : 3554.143\n##  ARIMA(2,0,1)(1,0,1)[12] with zero mean     : Inf\n##  ARIMA(2,0,2)            with zero mean     : 3546.184\n##  ARIMA(2,0,2)(0,0,1)[12] with zero mean     : 3548.584\n##  ARIMA(2,0,2)(1,0,0)[12] with zero mean     : 3548.589\n##  ARIMA(2,0,3)            with zero mean     : 3548.597\n##  ARIMA(3,0,0)            with zero mean     : 3551.834\n##  ARIMA(3,0,0)(0,0,1)[12] with zero mean     : 3553.7\n##  ARIMA(3,0,0)(1,0,0)[12] with zero mean     : 3553.683\n##  ARIMA(3,0,0)(1,0,1)[12] with zero mean     : Inf\n##  ARIMA(3,0,1)            with zero mean     : 3548.871\n##  ARIMA(3,0,1)(0,0,1)[12] with zero mean     : Inf\n##  ARIMA(3,0,1)(1,0,0)[12] with zero mean     : 3551.285\n##  ARIMA(3,0,2)            with zero mean     : 3548.575\n##  ARIMA(4,0,0)            with zero mean     : 3552.398\n##  ARIMA(4,0,0)(0,0,1)[12] with zero mean     : 3554.534\n##  ARIMA(4,0,0)(1,0,0)[12] with zero mean     : 3554.529\n##  ARIMA(4,0,1)            with zero mean     : 3550.555\n##  ARIMA(5,0,0)            with zero mean     : 3553.42\n## \n## \n## \n##  Best model: ARIMA(1,0,2)            with zero mean## Series: estacion.ts \n## ARIMA(1,0,2) with zero mean \n## \n## Coefficients:\n##          ar1      ma1     ma2\n##       0.9023  -1.5331  0.6856\n## s.e.  0.0701   0.1188  0.1099\n## \n## sigma^2 = 1.417e+23:  log likelihood = -1767.66\n## AIC=3543.33   AICc=3544.02   BIC=3551.9"},{"path":"modelo-arima.html","id":"analisis","chapter":"Capitulo 5 Modelo ARIMA","heading":"5.3 Analisis","text":"","code":""},{"path":"modelo-arima.html","id":"prediccion-del-modelo","chapter":"Capitulo 5 Modelo ARIMA","heading":"5.3.1 Prediccion del Modelo","text":"Utilicemos nuestro modelo ARIMA para pronosticar los siguintes 12 meses de saldo en las cuentas del banco.Ahora hagamos las validaciones del modelo","code":"## [1] \"Veamos mas en detalle la prediccion de los valores\"## [1] \"Valores de la prediccion:\"##               Jan          Feb          Mar          Apr          May\n## 2023                                                     165668225906\n## 2024 -33590498111 -30309584374 -27349130158 -24677834944             \n##               Jun          Jul          Aug          Sep          Oct\n## 2023 -68971819772 -62235075641 -56156335340 -50671329091 -45722064595\n## 2024                                                                 \n##               Nov          Dec\n## 2023 -41256213885 -37226560069\n## 2024## [1] \"Error in solve.default(res$hessian * n.used, A) :\\nLapack routine dgesv: system is exactly singular: U[1,1] = 0\"## [1] \"Al aplicar la funcion tso para encontrar los outliers, nos arroja un error de singularidad, por lo que descarta el uso de diferenciacion para convertir el ts a estacionario.\""},{"path":"modelo-arima.html","id":"diferenciacion-logaritmica","chapter":"Capitulo 5 Modelo ARIMA","heading":"5.3.2 Diferenciacion Logaritmica","text":"","code":"\n# Genero mi objeto ts para el analisis\nindice.ts <- ts(datos$Saldo, start = c(2018,1), frequency = 12)\nplot(indice.ts)"},{"path":"modelo-arima.html","id":"diferenciacion-por-logaritmo","chapter":"Capitulo 5 Modelo ARIMA","heading":"5.3.2.1 Diferenciacion por Logaritmo","text":"Finalmente podemos recharar la h0 ya que p<0.05","code":"\nmits <- log(indice.ts)\n\n# Prueba de Estacionariedad\nadf.test(mits)## \n##  Augmented Dickey-Fuller Test\n## \n## data:  mits\n## Dickey-Fuller = -1.9544, Lag order = 3, p-value = 0.5934\n## alternative hypothesis: stationary\nprint(\"Continua siendo No-Estacionaria. Procederemos a diferenciarla\")## [1] \"Continua siendo No-Estacionaria. Procederemos a diferenciarla\"\nmmits <- diff(mits)\nadf.test(mmits)## Warning in adf.test(mmits): p-value smaller than printed p-value## \n##  Augmented Dickey-Fuller Test\n## \n## data:  mmits\n## Dickey-Fuller = -5.3844, Lag order = 3, p-value = 0.01\n## alternative hypothesis: stationary\ntsdisplay(mmits)\nadf.test(mmits)## Warning in adf.test(mmits): p-value smaller than printed p-value## \n##  Augmented Dickey-Fuller Test\n## \n## data:  mmits\n## Dickey-Fuller = -5.3844, Lag order = 3, p-value = 0.01\n## alternative hypothesis: stationary"},{"path":"modelo-arima.html","id":"procedamos-a-calular-el-modelo-arima-mas-optimo","chapter":"Capitulo 5 Modelo ARIMA","heading":"5.3.2.2 Procedamos a calular el modelo Arima mas optimo:","text":"","code":"\nmimod <- auto.arima(mmits, trace = T)## \n##  ARIMA(2,0,2)(1,0,1)[12] with non-zero mean : Inf\n##  ARIMA(0,0,0)            with non-zero mean : -65.53048\n##  ARIMA(1,0,0)(1,0,0)[12] with non-zero mean : -78.58046\n##  ARIMA(0,0,1)(0,0,1)[12] with non-zero mean : -88.02136\n##  ARIMA(0,0,0)            with zero mean     : -66.66478\n##  ARIMA(0,0,1)            with non-zero mean : -90.27983\n##  ARIMA(0,0,1)(1,0,0)[12] with non-zero mean : -88.01915\n##  ARIMA(0,0,1)(1,0,1)[12] with non-zero mean : Inf\n##  ARIMA(1,0,1)            with non-zero mean : -88.46913\n##  ARIMA(0,0,2)            with non-zero mean : -88.62053\n##  ARIMA(1,0,0)            with non-zero mean : -80.81767\n##  ARIMA(1,0,2)            with non-zero mean : -86.98859\n##  ARIMA(0,0,1)            with zero mean     : -83.77733\n## \n##  Best model: ARIMA(0,0,1)            with non-zero mean\nmimod## Series: mmits \n## ARIMA(0,0,1) with non-zero mean \n## \n## Coefficients:\n##           ma1    mean\n##       -0.6742  0.0182\n## s.e.   0.0846  0.0047\n## \n## sigma^2 = 0.01291:  log likelihood = 48.34\n## AIC=-90.69   AICc=-90.28   BIC=-84.26\n# Hagamos la prediccion de los siguientes 12 meses.\nmifore <- forecast(mimod, h=12)\nmifore##          Point Forecast       Lo 80     Hi 80      Lo 95     Hi 95\n## May 2023     0.06780752 -0.07779118 0.2134062 -0.1548665 0.2904815\n## Jun 2023     0.01815148 -0.15744629 0.1937492 -0.2504021 0.2867051\n## Jul 2023     0.01815148 -0.15744629 0.1937492 -0.2504021 0.2867051\n## Aug 2023     0.01815148 -0.15744629 0.1937492 -0.2504021 0.2867051\n## Sep 2023     0.01815148 -0.15744629 0.1937492 -0.2504021 0.2867051\n## Oct 2023     0.01815148 -0.15744629 0.1937492 -0.2504021 0.2867051\n## Nov 2023     0.01815148 -0.15744629 0.1937492 -0.2504021 0.2867051\n## Dec 2023     0.01815148 -0.15744629 0.1937492 -0.2504021 0.2867051\n## Jan 2024     0.01815148 -0.15744629 0.1937492 -0.2504021 0.2867051\n## Feb 2024     0.01815148 -0.15744629 0.1937492 -0.2504021 0.2867051\n## Mar 2024     0.01815148 -0.15744629 0.1937492 -0.2504021 0.2867051\n## Apr 2024     0.01815148 -0.15744629 0.1937492 -0.2504021 0.2867051\nplot(mifore)"},{"path":"modelo-arima.html","id":"revisemos-el-efecto-de-los-outliers","chapter":"Capitulo 5 Modelo ARIMA","heading":"5.3.3 Revisemos el efecto de los outliers:","text":"La gráfica de Outliers muestra los 10 valores que difieren significativamente del patrón general de la serie de tiempo.","code":"\noutliers_excess_ts <- tso(mmits)## Warning in locate.outliers.iloop(resid = resid, pars = pars, cval = cval, :\n## stopped when 'maxit.iloop' was reached\n\n## Warning in locate.outliers.iloop(resid = resid, pars = pars, cval = cval, :\n## stopped when 'maxit.iloop' was reached\n\n## Warning in locate.outliers.iloop(resid = resid, pars = pars, cval = cval, :\n## stopped when 'maxit.iloop' was reached\n\n## Warning in locate.outliers.iloop(resid = resid, pars = pars, cval = cval, :\n## stopped when 'maxit.iloop' was reached## Warning in locate.outliers.oloop(y = y, fit = fit, types = types, cval = cval,\n## : stopped when 'maxit.oloop = 4' was reached\noutliers_excess_ts## Series: mmits \n## Regression with ARIMA(2,0,0) errors \n## \n## Coefficients:\n##           ar1      ar2  intercept     TC19    LS20     TC21    AO22     TC23\n##       -1.4393  -0.6456     0.0292  -0.0961  0.2422  -0.6172  0.7346  -0.4820\n## s.e.   0.0981   0.0953     0.0038   0.0226  0.0286   0.0719  0.0863   0.0285\n##         AO25     TC28    LS31     LS35    TC44\n##       0.6319  -0.2536  -0.180  -0.0733  0.0521\n## s.e.  0.0492   0.0384   0.022   0.0130  0.0145\n## \n## sigma^2 = 0.002827:  log likelihood = 101.48\n## AIC=-174.96   AICc=-166.21   BIC=-144.96\n## \n## Outliers:\n##    type ind    time  coefhat   tstat\n## 1    TC  19 2019:08 -0.09610  -4.262\n## 2    LS  20 2019:09  0.24224   8.461\n## 3    TC  21 2019:10 -0.61716  -8.580\n## 4    AO  22 2019:11  0.73456   8.516\n## 5    TC  23 2019:12 -0.48196 -16.888\n## 6    AO  25 2020:02  0.63191  12.842\n## 7    TC  28 2020:05 -0.25360  -6.606\n## 8    LS  31 2020:08 -0.17998  -8.190\n## 9    LS  35 2020:12 -0.07327  -5.620\n## 10   TC  44 2021:09  0.05214   3.608\nplot(outliers_excess_ts)"},{"path":"modelo-arima.html","id":"por-ultimo-haremos-un-check-de-los-residuos.","chapter":"Capitulo 5 Modelo ARIMA","heading":"5.3.3.1 Por ultimo haremos un check de los residuos.","text":"Con esta grafica podemos concluir que los residuales presentan una distribucion normal, que los errores residuales se mantienen dentro del rango de significancia aceptable (0.95) y por medio de un t-test pudimos corroborar que la media de los residuos es 0.","code":"\ncheckresiduals(mimod)## \n##  Ljung-Box test\n## \n## data:  Residuals from ARIMA(0,0,1) with non-zero mean\n## Q* = 2.0482, df = 12, p-value = 0.9993\n## \n## Model df: 1.   Total lags used: 13\nresiduales <- mimod$residuals\n\nt.test(residuales, alternative='two.sided', conf.level=0.95, mu=0)## \n##  One Sample t-test\n## \n## data:  residuales\n## t = 0.047608, df = 62, p-value = 0.9622\n## alternative hypothesis: true mean is not equal to 0\n## 95 percent confidence interval:\n##  -0.02770451  0.02905634\n## sample estimates:\n##    mean of x \n## 0.0006759195"},{"path":"modelos-logaritmicos-prophet-y-otros.html","id":"modelos-logaritmicos-prophet-y-otros","chapter":"Capitulo 6 Modelos Logaritmicos, Prophet y otros","heading":"Capitulo 6 Modelos Logaritmicos, Prophet y otros","text":"Transformación logarítmica:\nLa transformación logarítmica es útil cuando hay una tendencia exponencial en la serie de tiempo y la varianza aumenta con el nivel de la serie. Aplicar la transformación logarítmica puede ayudar reducir la tendencia y estabilizar la varianza.se Puede utilizar la función log() para aplicar esta transformación:Transformación de raíz cuadrada:\nLa transformación de raíz cuadrada se utiliza cuando la varianza aumenta con el nivel de la serie de tiempo. Al aplicar esta transformación, se reduce la dispersión de los valores más altos y se estabiliza la varianza. Puedes utilizar la función sqrt() para aplicar esta transformación:Metodología de Holt-Winters:Para aplicar la metodología de Holt-Winters y suavizamiento exponencial la serie de tiempo indice.ts, podemos utilizar las funciones correspondientes de R. La metodología de Holt-Winters es adecuada para modelar series de tiempo con componentes de tendencia y estacionalidad.\nEl resultado del modelo Holt-Winters proporciona las componentes de tendencia (trend), estacionalidad (seasonal) y residuos (residuals). Estas componentes ayudan comprender los patrones y la estructura de la serie de tiempo.La línea azul representa la serie de tiempo original, es decir, los valores observados de la variable lo largo del tiempo.\nLa línea roja representa la versión suavizada de la serie de tiempo, obtenida mediante el suavizamiento exponencial simple.\nEl suavizamiento exponencial simple utiliza un promedio ponderado de los valores anteriores para generar la versión suavizada. medida que avanzas en el tiempo, la línea roja muestra una representación suavizada de la tendencia general de la serie.\nLa versión suavizada ayuda eliminar el ruido y las fluctuaciones más pequeñas presentes en la serie original. Esto permite visualizar mejor la tendencia subyacente en los datos.\nObserva cómo la línea roja se ajusta la tendencia general de la serie original. En general, el suavizamiento exponencial simple puede ser útil para identificar patrones de tendencia en los datos y proporcionar una representación más clara de la evolución de la variable lo largo del tiempo.Modelos estacionarios en series de tiempoEnfoque de regresión lineal clásico:Algoritmo Facebook´s Prophet:","code":"\n# Aplicar la transformación logarítmica\nserie_log <- log(indice.ts)\n\n# Graficar la serie transformada\nplot(serie_log, main = \"Serie de tiempo transformada (Logarítmica)\")\n# Aplicar la transformación de raíz cuadrada\nserie_sqrt <- sqrt(indice.ts)\n\n# Graficar la serie transformada\nplot(serie_sqrt, main = \"Serie de tiempo transformada (Raíz Cuadrada)\")\nlibrary(forecast)\n\n# Aplicar la metodología de Holt-Winters\nhw_model <- HoltWinters(indice.ts)\n\n# Obtener el pronóstico utilizando el modelo Holt-Winters\nhw_forecast <- forecast(hw_model, h = 12)  # Pronóstico para los próximos 12 periodos\n\n# Graficar la serie de tiempo y el pronóstico\nplot(hw_forecast, main = \"Pronóstico utilizando Holt-Winters\")\n# Aplicar suavizamiento exponencial simple\nses_model <- ses(indice.ts)\n\n# Obtener la serie suavizada\nhorizon <- 6  # Pronosticar 6 períodos hacia adelante\nses_smooth <- forecast(ses_model, h = horizon)$mean\n\n# Graficar la serie de tiempo y la versión suavizada\nplot(indice.ts, type = \"l\", col = \"#00a0dc\", main = \"Suavizamiento exponencial simple\")\nlines(ses_smooth, col = \"red\")\n# Crear una variable de tiempo numérica\ntime <- 1:length(indice.ts)\n\n# Ajustar un modelo lineal y estacionario\nlm_model <- lm(indice.ts ~ time)\n\n# Obtener los coeficientes del modelo\ncoef_lm <- coef(lm_model)\n\n# Graficar la serie de tiempo y el modelo ajustado\nplot(indice.ts, type = \"l\", col = \"#00a0dc\", main = \"Ajuste lineal y estacionario\")\nlines(fitted(lm_model), col = \"red\")\n# Instalar y cargar la librería prophet\n#install.packages(\"prophet\")\nlibrary(prophet)\n\n# Crear un dataframe con la serie de tiempo\ndf <- data.frame(ds = as.Date(datos$Periodo),\n                 y = datos$Saldo)\n\n# Ajustar el modelo Prophet\nprophet_model <- prophet(df)\n\n# Realizar un pronóstico para los próximos 12 meses\nfuture <- make_future_dataframe(prophet_model, periods = 12)\nforecast <- predict(prophet_model, future)\n\n# Graficar el pronóstico\nplot(prophet_model, forecast)"},{"path":"modelos-de-redes-neuronales-recurrentes.html","id":"modelos-de-redes-neuronales-recurrentes","chapter":"Capitulo 7 Modelos de Redes Neuronales Recurrentes","heading":"Capitulo 7 Modelos de Redes Neuronales Recurrentes","text":"","code":""},{"path":"modelos-de-redes-neuronales-recurrentes.html","id":"elman","chapter":"Capitulo 7 Modelos de Redes Neuronales Recurrentes","heading":"7.1 Elman","text":"","code":""},{"path":"modelos-de-redes-neuronales-recurrentes.html","id":"carga-de-datos","chapter":"Capitulo 7 Modelos de Redes Neuronales Recurrentes","heading":"7.1.1 Carga de Datos","text":"Para empezar el ejercicio, vamos cargar nuevamente nuestros datos fuente. Esto con el fin de evitar cualquier transformacion anterior que pueda distorsionar la serie de tiempo original.","code":"\nlibrary(forecast)\nlibrary(timsac)\nlibrary(ggplot2)\nlibrary(changepoint)\nlibrary(readxl)\nlibrary(RSNNS)\nlibrary(quantmod)## Loading required package: xts## Loading required package: TTR\ndatos <- read_excel(\"./fuente/BASE_Clientes.xlsx\")\ndatos## # A tibble: 643,570 × 8\n##    Periodo Sub_Tipo N_Clientes DIAS_DE_MORA     Saldo Genero grupo_actividad_eco\n##    <chr>   <chr>         <dbl>        <dbl>     <dbl> <chr>  <chr>              \n##  1 2018-01 CDC               4            0 15824105. Femen… Dependiente privado\n##  2 2018-01 CDC               1            0  6810373. Femen… Dependiente privado\n##  3 2018-01 CDC               6            6 28819502. Femen… Dependiente privado\n##  4 2018-01 CDC              12           63 81343674. Femen… Dependiente privado\n##  5 2018-01 CDC               1           21  7524344. Femen… Dependiente privado\n##  6 2018-01 CDC               4            0 12974213. Femen… Dependiente privado\n##  7 2018-01 CDC               3            1 21348609. Femen… Dependiente privado\n##  8 2018-01 CDC               2            0 11475858. Femen… Dependiente privado\n##  9 2018-01 CDC              10           38 60012355. Femen… Dependiente privado\n## 10 2018-01 CDC               1            0  9034715. Femen… Dependiente privado\n## # ℹ 643,560 more rows\n## # ℹ 1 more variable: Cuidad_res <chr>\n# Cambio el tipo de dato de la columna temporal(Periodo)\ndatos$Periodo <- as.Date(paste0(datos$Periodo, \"-01\"))\n\n# Consolido el df en funcion de la variable de interes (Saldo)\ndatos <- aggregate(Saldo ~ Periodo, data = datos, sum)\n\n# Genero mi objeto ts para el analisis\nindice.ts <- ts(datos$Saldo, start = c(2018,1), frequency = 12)\nindice.ts##               Jan          Feb          Mar          Apr          May\n## 2018 2.562223e+12 2.601532e+12 2.653315e+12 2.717915e+12 2.852608e+12\n## 2019 3.732137e+12 3.810740e+12 3.923380e+12 3.995810e+12 4.092819e+12\n## 2020 2.022405e+12 4.295886e+12 4.304185e+12 4.195404e+12 4.177219e+12\n## 2021 4.369300e+12 4.478002e+12 4.595047e+12 4.627874e+12 4.705783e+12\n## 2022 6.351501e+12 6.555819e+12 6.739823e+12 6.925001e+12 7.132542e+12\n## 2023 8.083445e+12 7.951094e+12 7.812947e+12 7.765132e+12             \n##               Jun          Jul          Aug          Sep          Oct\n## 2018 2.986446e+12 3.102493e+12 3.196138e+12 3.272388e+12 3.394038e+12\n## 2019 4.164386e+12 4.198177e+12 4.267921e+12 4.307340e+12 4.113506e+12\n## 2020 4.164434e+12 4.082507e+12 4.067948e+12 4.120926e+12 4.182877e+12\n## 2021 4.846473e+12 4.983882e+12 5.091594e+12 5.385784e+12 5.730839e+12\n## 2022 7.435792e+12 7.529328e+12 7.670212e+12 7.789046e+12 7.903472e+12\n## 2023                                                                 \n##               Nov          Dec\n## 2018 3.617129e+12 3.690229e+12\n## 2019 4.314034e+12 4.455499e+12\n## 2020 4.344709e+12 4.384188e+12\n## 2021 5.955740e+12 6.164705e+12\n## 2022 8.115444e+12 8.154174e+12\n## 2023"},{"path":"modelos-de-redes-neuronales-recurrentes.html","id":"creacion-de-la-ts-normalizada","chapter":"Capitulo 7 Modelos de Redes Neuronales Recurrentes","heading":"7.1.2 Creacion de la TS normalizada","text":"Una vez hecho esto, convierto mis datos una time series (Z). Como estaremos trabajando con Redes Neuronales Recurrentes, normalizaremos la serie para que sus datos fluctuen entre 0 y 1.","code":"\nZ <- as.ts(indice.ts,F)\nS <- (Z-min(Z))/(max(Z)-min(Z))\nplot(S)"},{"path":"modelos-de-redes-neuronales-recurrentes.html","id":"division-de-la-serie.","chapter":"Capitulo 7 Modelos de Redes Neuronales Recurrentes","heading":"7.1.3 Division de la serie.","text":"Ahora dividiremos el el numero de filas totales para trabajar con nuestros sets de entrenamiento y testing.","code":"\nlineas_totales <-length(S)\nt_train <- round(lineas_totales*0.75, digits=0)\nl_train <- 0:(t_train-1) \nt_test <- (t_train):lineas_totales\nt_test##  [1] 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64"},{"path":"modelos-de-redes-neuronales-recurrentes.html","id":"creacion-de-nodos.","chapter":"Capitulo 7 Modelos de Redes Neuronales Recurrentes","heading":"7.1.4 Creacion de nodos.","text":"Ahora crearemos un df con los nodos que adelantaran un valor en el futuroActo seguido, especificaremos los inputs y outputs de la red","code":"\ny <- as.zoo(S)\nx1 <- Lag(y, k = 1)\nx2 <- Lag(y, k = 2)\nx3 <- Lag(y, k = 3)\nx4 <- Lag(y, k = 4)\nx5 <- Lag(y, k = 5)\nx6 <- Lag(y, k = 6)\nx7 <- Lag(y, k = 7)\nx8 <- Lag(y, k = 8)\nx9 <- Lag(y, k = 9)\nx10 <- Lag(y, k = 10)\nx11 <- Lag(y, k = 11)\nx12 <- Lag(y, k = 12)\nslogN <- cbind(y,x1,x2,x3,x4,x5,x6,x7,x8,x9,x10,x11,x12)\n# Elimenemos los valores desplazados\nslogN <- slogN[-(1:12),]\ninputs <- slogN[,2:13]\noutputs <- slogN[,1]"},{"path":"modelos-de-redes-neuronales-recurrentes.html","id":"entrenamiento-del-modelo-elman","chapter":"Capitulo 7 Modelos de Redes Neuronales Recurrentes","heading":"7.1.5 Entrenamiento del modelo Elman","text":"Con la informaion en su lugar, procedamos crear la red de Elman. Despues de varias pruebas, el numero de neuronas y profundidad queda en 110 y 5 respectivamente.Veamos como evoluciona el errorAhora procederemos hacer la la prediccion con el resto de terminos de la serie:Procedemos hacer las predicciones con nuestra redDesnormalizamos datosveamos los valores","code":"\nset.seed(42)\nfit <- fit<-elman(inputs[t_train],outputs[t_train],size=c(110,5),learnFuncParams=c(0.1),maxit=100000)\nplotIterativeError(fit, main = \"Error Iterativo para neuronas 110,5\")\ny <- as.vector(outputs[-t_test])\nplot(y,type=\"l\")\npred <- predict(fit, inputs[-t_test])\nlines(pred,col = \"red\")\npredictions <- predict(fit,inputs[-t_train])\nmod_elman <- predictions*(max(Z)-min(Z))+min(Z)\nmod_elman##                  [,1]\n## Jan 2019 6.162340e+12\n## Feb 2019 5.689504e+12\n## Mar 2019 5.357780e+12\n## Apr 2019 5.185718e+12\n## May 2019 5.130582e+12\n## Jun 2019 5.080404e+12\n## Jul 2019 5.083780e+12\n## Aug 2019 5.127912e+12\n## Sep 2019 5.149160e+12\n## Oct 2019 5.186750e+12\n## Nov 2019 5.231140e+12\n## Dec 2019 5.260487e+12\n## Jan 2020 5.266360e+12\n## Feb 2020 5.352791e+12\n## Mar 2020 5.497074e+12\n## Apr 2020 5.287112e+12\n## May 2020 5.651816e+12\n## Jun 2020 5.150608e+12\n## Jul 2020 5.294518e+12\n## Aug 2020 5.452413e+12\n## Sep 2020 5.309498e+12\n## Oct 2020 5.267031e+12\n## Nov 2020 5.588612e+12\n## Dec 2020 5.326231e+12\n## Jan 2021 5.264540e+12\n## Feb 2021 5.495523e+12\n## Mar 2021 5.388697e+12\n## Apr 2021 5.367434e+12\n## May 2021 5.368399e+12\n## Jun 2021 5.363104e+12\n## Jul 2021 5.369137e+12\n## Aug 2021 5.400116e+12\n## Sep 2021 5.406626e+12\n## Oct 2021 5.420363e+12\n## Nov 2021 5.441833e+12\n## Dec 2021 5.453365e+12\n## Jan 2022 5.479368e+12\n## Feb 2022 5.561483e+12\n## Mar 2022 5.700627e+12\n## Apr 2022 5.861865e+12\n## May 2022 6.044024e+12\n## Jun 2022 6.233664e+12\n## Jul 2022 6.373705e+12\n## Aug 2022 6.465632e+12\n## Sep 2022 6.542706e+12\n## Oct 2022 6.535043e+12\n## Nov 2022 6.523112e+12\n## Jan 2023 6.533344e+12\n## Feb 2023 6.531746e+12\n## Mar 2023 6.557522e+12\n## Apr 2023 6.571427e+12\nx <- 1:(lineas_totales+length(mod_elman))\ny <- c(as.vector(Z),mod_elman)\nplot(x[1:lineas_totales], y[1:lineas_totales],col = \"blue\", type=\"l\")\nlines( x[(lineas_totales):length(x)], y[(lineas_totales):length(x)], col=\"red\")"},{"path":"modelos-de-redes-neuronales-recurrentes.html","id":"jordan","chapter":"Capitulo 7 Modelos de Redes Neuronales Recurrentes","heading":"7.2 Jordan","text":"","code":""},{"path":"modelos-de-redes-neuronales-recurrentes.html","id":"entrenamiento-del-modelo-jordan","chapter":"Capitulo 7 Modelos de Redes Neuronales Recurrentes","heading":"7.2.1 Entrenamiento del modelo Jordan","text":"Como ya tenemos la tsnormalizada (S) y dividida entre training y testing, procedemos entrenar el algoritmo con Jordan:","code":"\nfit<-jordan(inputs[t_train],\n    outputs[t_train],\n    size=16,\n    learnFuncParams=c(0.01),\n    maxit=100000)"},{"path":"modelos-de-redes-neuronales-recurrentes.html","id":"error-iterativo","chapter":"Capitulo 7 Modelos de Redes Neuronales Recurrentes","heading":"7.2.2 Error Iterativo","text":"Ploteamos el error iterativo de 16 neuronas:Veamos ahora como se comporta el error:","code":"\nplotIterativeError(fit, main = \"Error iterativo para 16 neuronas\")\ny <- as.vector(outputs[-t_test])\nplot(y,type=\"l\")\npred <- predict(fit, inputs[-t_test])\nlines(pred,col = \"red\")\npredictions <- predict(fit,inputs[-t_train])\nmod_jordan <- predictions*(max(Z)-min(Z))+min(Z)\nmod_jordan##                  [,1]\n## Jan 2019 1.148488e+13\n## Feb 2019 1.141995e+13\n## Mar 2019 1.139538e+13\n## Apr 2019 1.134228e+13\n## May 2019 1.129364e+13\n## Jun 2019 1.128617e+13\n## Jul 2019 1.130867e+13\n## Aug 2019 1.133650e+13\n## Sep 2019 1.133135e+13\n## Oct 2019 1.129645e+13\n## Nov 2019 1.137222e+13\n## Dec 2019 1.128734e+13\n## Jan 2020 1.126138e+13\n## Feb 2020 1.186075e+13\n## Mar 2020 1.069085e+13\n## Apr 2020 1.085286e+13\n## May 2020 1.086688e+13\n## Jun 2020 1.095609e+13\n## Jul 2020 1.168398e+13\n## Aug 2020 1.113439e+13\n## Sep 2020 1.118931e+13\n## Oct 2020 1.121318e+13\n## Nov 2020 1.079012e+13\n## Dec 2020 1.189949e+13\n## Jan 2021 1.087328e+13\n## Feb 2021 1.122287e+13\n## Mar 2021 1.127449e+13\n## Apr 2021 1.128306e+13\n## May 2021 1.129050e+13\n## Jun 2021 1.134053e+13\n## Jul 2021 1.136667e+13\n## Aug 2021 1.135414e+13\n## Sep 2021 1.138047e+13\n## Oct 2021 1.134630e+13\n## Nov 2021 1.137113e+13\n## Dec 2021 1.146756e+13\n## Jan 2022 1.154174e+13\n## Feb 2022 1.163130e+13\n## Mar 2022 1.168323e+13\n## Apr 2022 1.170141e+13\n## May 2022 1.173339e+13\n## Jun 2022 1.176145e+13\n## Jul 2022 1.179491e+13\n## Aug 2022 1.183078e+13\n## Sep 2022 1.185394e+13\n## Oct 2022 1.190688e+13\n## Nov 2022 1.191501e+13\n## Jan 2023 1.193379e+13\n## Feb 2023 1.197258e+13\n## Mar 2023 1.198780e+13\n## Apr 2023 1.195651e+13"},{"path":"modelos-de-redes-neuronales-recurrentes.html","id":"resultados-de-la-prediccion","chapter":"Capitulo 7 Modelos de Redes Neuronales Recurrentes","heading":"7.2.3 Resultados de la prediccion","text":"Ahora veamoslo en la grafica","code":"\nx <- 1:(lineas_totales+length(mod_jordan))\ny <- c(as.vector(Z),mod_jordan)\nplot(x[1:lineas_totales], y[1:lineas_totales],col = \"blue\", type=\"l\")\nlines( x[(lineas_totales):length(x)], y[(lineas_totales):length(x)], col=\"red\")"}]
